diff --git a/cpp/conversation.h b/cpp/conversation.h
index 82332ae..8d8a933 100644
--- a/cpp/conversation.h
+++ b/cpp/conversation.h
@@ -195,9 +195,12 @@ class Conversation {
   void AppendReplyHeader(std::string role) { this->messages.push_back({role}); }
 
   void FinishReply(std::string msg) {
-    ICHECK_NE(this->messages.size(), 0);
-    ICHECK_EQ(this->messages.back().size(), 1) << "Already assigned";
-    this->messages.back().push_back(msg);
+    // When using prefill_with_embed without embed_text() first:
+    // InternalError: Check failed: this->messages.size() != 0 (0 vs. 0)
+    /*ICHECK_NE(this->messages.size(), 0);
+    ICHECK_EQ(this->messages.back().size(), 1) << "Already assigned";*/
+    if( this->messages.size() > 0 && this->messages.back().size() == 0 )
+	 this->messages.back().push_back(msg);
   }
 
   void Reset() { this->messages.resize(this->offset); }
diff --git a/cpp/llm_chat.cc b/cpp/llm_chat.cc
index 520f381..964f737 100644
--- a/cpp/llm_chat.cc
+++ b/cpp/llm_chat.cc
@@ -671,8 +671,8 @@ class LLMChat {
     NDArray input_data = this->GetInputTokenNDArray(prompt_tokens);
     ObjectRef embedding = ft_.embed_func_(ft_.CopyToWorker0(input_data), params_);
 
-    int32_t new_seq_len = total_seq_len_ + token_len;
-    total_seq_len_ = new_seq_len;
+    //int32_t new_seq_len = total_seq_len_ + token_len;
+    //total_seq_len_ = new_seq_len;
 
     auto tend = std::chrono::high_resolution_clock::now();
 
@@ -694,8 +694,20 @@ class LLMChat {
     if (embedding.Shape().size() == 0) {
       return;
     }
+    
+    // File "/opt/mlc-llm/cpp/llm_chat.cc", line 923
+    // InternalError: Check failed: (!stop_triggered_) is false: Cannot call process when it is stopped
+    output_ids_.clear();
+    appeared_token_ids_.clear();
+    output_message_.clear();
+    stop_triggered_ = false;
+    
     auto tstart = std::chrono::high_resolution_clock::now();
     int64_t token_len = embedding.Shape()[1];
+    
+    // mlc-llm/3rdparty/tvm/src/runtime/relax_vm/lm_support.cc:79
+    // InternalError: Check failed: "Requested shape do not match the filled count"
+    total_seq_len_ += token_len; 
     NDArray logits_on_device = this->ForwardEmbeddings(embedding, total_seq_len_);
 
     if (!decode_next_token) {
@@ -928,26 +940,26 @@ class LLMChat {
     if (!stop_triggered_) {
       output_ids_.push_back(next_token);
       appeared_token_ids_.insert(next_token);
-    }
 
-    output_message_ = tokenizer_->Decode(output_ids_);
-
-    if (!conversation_.stop_str.empty()) {
-      size_t stop_pos = output_message_.rfind(conversation_.stop_str);
-      if (stop_pos != std::string::npos) {
-        stop_triggered_ = true;
-        if (ft_.support_backtracking_kv_) {
-          // back tracking, find the first set of token that is smaller
-          // than the length
-          size_t backoff = 0;
-          for (; backoff < output_ids_.size(); ++backoff) {
-            output_ids_.pop_back();
-            output_message_ = tokenizer_->Decode(output_ids_);
-            if (output_message_.length() <= stop_pos) break;
+      output_message_ = tokenizer_->Decode(output_ids_);
+
+      if (!conversation_.stop_str.empty()) {
+        size_t stop_pos = output_message_.rfind(conversation_.stop_str);
+        if (stop_pos != std::string::npos) {
+          stop_triggered_ = true;
+          if (ft_.support_backtracking_kv_) {
+            // back tracking, find the first set of token that is smaller
+            // than the length
+            size_t backoff = 0;
+            for (; backoff < output_ids_.size(); ++backoff) {
+              output_ids_.pop_back();
+              output_message_ = tokenizer_->Decode(output_ids_);
+              if (output_message_.length() <= stop_pos) break;
+            }
+            // resize kv to remove the context
+            ft_.fkvcache_array_popn_(kv_cache_, backoff);
+            total_seq_len_ -= backoff;
           }
-          // resize kv to remove the context
-          ft_.fkvcache_array_popn_(kv_cache_, backoff);
-          total_seq_len_ -= backoff;
         }
       }
     }
diff --git a/mlc_llm/core.py b/mlc_llm/core.py
index 8eb9949..a9d1822 100644
--- a/mlc_llm/core.py
+++ b/mlc_llm/core.py
@@ -394,7 +394,10 @@ def mod_transform_before_build(
         has_cutlass = tvm.get_global_func("relax.ext.cutlass", True)
 
         if has_cutlass and not args.no_cutlass_attn:
-            mod["prefill"] = rewrite_attention(mod["prefill"])
+            if args.sep_embed:
+                mod["prefill_with_embed"] = rewrite_attention(mod["prefill_with_embed"])
+            else:
+                mod["prefill"] = rewrite_attention(mod["prefill"])
             mod["decode"] = rewrite_attention(mod["decode"])
             patterns += get_patterns_with_prefix("cutlass.attention")
 
