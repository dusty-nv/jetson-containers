diff --git a/sgl-kernel/CMakeLists.txt b/sgl-kernel/CMakeLists.txt
--- a/sgl-kernel/CMakeLists.txt	(revision 904655c5fd746577b24ebc0b35bf6fe49daf649a)
+++ b/sgl-kernel/CMakeLists.txt	(date 1761122018260)
@@ -137,8 +137,8 @@
 option(ENABLE_BELOW_SM90 "Enable below SM90" ON)

 if (CMAKE_SYSTEM_PROCESSOR MATCHES "aarch64")
-    set(ENABLE_BELOW_SM90 OFF)
-    message(STATUS "For aarch64, disable gencode below SM90 by default")
+    set(ENABLE_BELOW_SM90 ON)
+    message(STATUS "For aarch64, explicitly enable gencode below SM90")
 endif()

 include_directories(
@@ -152,7 +152,7 @@
     "-O3"
     "-Xcompiler"
     "-fPIC"
-    "-gencode=arch=compute_90,code=sm_90"
+    "-gencode=arch=compute_87,code=sm_87"
     "-std=c++17"
     "-DFLASHINFER_ENABLE_F16"
     "-DCUTE_USE_PACKED_TUPLE=1"
@@ -221,42 +221,7 @@

 if (ENABLE_BELOW_SM90)
     list(APPEND SGL_KERNEL_CUDA_FLAGS
-        "-gencode=arch=compute_80,code=sm_80"
-        "-gencode=arch=compute_89,code=sm_89"
-    )
-endif()
-
-if ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "12.8" OR SGL_KERNEL_ENABLE_SM100A)
-    list(APPEND SGL_KERNEL_CUDA_FLAGS
-        "-gencode=arch=compute_100a,code=sm_100a"
-        "-gencode=arch=compute_120a,code=sm_120a"
-    )
-
-    # refer sm_121, sm_110 and sm_101 description  https://github.com/pytorch/pytorch/pull/156176
-    if ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "13.0")
-        list(APPEND SGL_KERNEL_CUDA_FLAGS
-            "-gencode=arch=compute_103a,code=sm_103a"
-            "-gencode=arch=compute_110a,code=sm_110a"
-            "-gencode=arch=compute_121a,code=sm_121a"
-            "--compress-mode=size"
-        )
-    else()
-        list(APPEND SGL_KERNEL_CUDA_FLAGS
-            "-gencode=arch=compute_101a,code=sm_101a"
-        )
-    endif()
-endif()
-
-if ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "12.4")
-    set(SGL_KERNEL_ENABLE_FA3 ON)
-    list(APPEND SGL_KERNEL_CUDA_FLAGS
-        "-gencode=arch=compute_90a,code=sm_90a"
-    )
-endif()
-
-if ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "12.8" OR SGL_KERNEL_ENABLE_FP4)
-    list(APPEND SGL_KERNEL_CUDA_FLAGS
-        "-DENABLE_NVFP4=1"
+        "-gencode=arch=compute_87,code=sm_87"
     )
 endif()

@@ -370,32 +335,34 @@
     OUTPUT_NAME "common_ops"
     LIBRARY_OUTPUT_DIRECTORY "${CMAKE_CURRENT_BINARY_DIR}/sm90"
 )
+# Also install to sm100 directory for compatibility with older GPUs
+install(TARGETS common_ops_sm90_build LIBRARY DESTINATION sgl_kernel/sm100)

 # =========================== Common SM100+ Build ============================= #
 # Build SM100+ library with precise math (same namespace, different directory)
-Python_add_library(common_ops_sm100_build MODULE USE_SABI ${SKBUILD_SABI_VERSION} WITH_SOABI ${SOURCES})
+# Python_add_library(common_ops_sm100_build MODULE USE_SABI ${SKBUILD_SABI_VERSION} WITH_SOABI ${SOURCES})

-target_compile_definitions(common_ops_sm100_build PRIVATE
-    USE_FAST_MATH=0
-)
-target_compile_options(common_ops_sm100_build PRIVATE
-    $<$<COMPILE_LANGUAGE:CUDA>:${SGL_KERNEL_CUDA_FLAGS}>
-)
-target_include_directories(common_ops_sm100_build PRIVATE
-    ${repo-cutlass_SOURCE_DIR}/include
-    ${repo-cutlass_SOURCE_DIR}/tools/util/include
-    ${repo-flashinfer_SOURCE_DIR}/include
-    ${repo-flashinfer_SOURCE_DIR}/csrc
-    ${repo-mscclpp_SOURCE_DIR}/include
-    ${repo-cutlass_SOURCE_DIR}/examples/77_blackwell_fmha
-    ${repo-cutlass_SOURCE_DIR}/examples/common
-    ${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src
-)
+#target_compile_definitions(common_ops_sm100_build PRIVATE
+#    USE_FAST_MATH=0
+#)
+#target_compile_options(common_ops_sm100_build PRIVATE
+#    $<$<COMPILE_LANGUAGE:CUDA>:${SGL_KERNEL_CUDA_FLAGS}>
+#)
+#target_include_directories(common_ops_sm100_build PRIVATE
+#    ${repo-cutlass_SOURCE_DIR}/include
+#    ${repo-cutlass_SOURCE_DIR}/tools/util/include
+#    ${repo-flashinfer_SOURCE_DIR}/include
+#    ${repo-flashinfer_SOURCE_DIR}/csrc
+#    ${repo-mscclpp_SOURCE_DIR}/include
+#    ${repo-cutlass_SOURCE_DIR}/examples/77_blackwell_fmha
+#    ${repo-cutlass_SOURCE_DIR}/examples/common
+#    ${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src
+#)
 # Set output name and separate build directory to avoid conflicts
-set_target_properties(common_ops_sm100_build PROPERTIES
-    OUTPUT_NAME "common_ops"
-    LIBRARY_OUTPUT_DIRECTORY "${CMAKE_CURRENT_BINARY_DIR}/sm100"
-)
+#set_target_properties(common_ops_sm100_build PROPERTIES
+#    OUTPUT_NAME "common_ops"
+#    LIBRARY_OUTPUT_DIRECTORY "${CMAKE_CURRENT_BINARY_DIR}/sm100"
+#)

 find_package(Python3 COMPONENTS Interpreter REQUIRED)
 execute_process(
@@ -422,7 +389,7 @@
     ${CMAKE_CURRENT_BINARY_DIR}/mscclpp-build
 )
 target_link_libraries(common_ops_sm90_build PRIVATE ${TORCH_LIBRARIES} c10 cuda cublas cublasLt mscclpp_static)
-target_link_libraries(common_ops_sm100_build PRIVATE ${TORCH_LIBRARIES} c10 cuda cublas cublasLt mscclpp_static)
+#target_link_libraries(common_ops_sm100_build PRIVATE ${TORCH_LIBRARIES} c10 cuda cublas cublasLt mscclpp_static)

 # sparse flash attention
 target_compile_definitions(common_ops_sm90_build PRIVATE
@@ -430,17 +397,17 @@
     FLASHATTENTION_DISABLE_DROPOUT
     FLASHATTENTION_DISABLE_UNEVEN_K
 )
-target_compile_definitions(common_ops_sm100_build PRIVATE
-    FLASHATTENTION_DISABLE_BACKWARD
-    FLASHATTENTION_DISABLE_DROPOUT
-    FLASHATTENTION_DISABLE_UNEVEN_K
-)
+#target_compile_definitions(common_ops_sm100_build PRIVATE
+#    FLASHATTENTION_DISABLE_BACKWARD
+#    FLASHATTENTION_DISABLE_DROPOUT
+#    FLASHATTENTION_DISABLE_UNEVEN_K
+#)

 # Install to different subdirectories
 # CMake will find the built libraries in their respective LIBRARY_OUTPUT_DIRECTORY locations
 # and install them to the specified destinations
 install(TARGETS common_ops_sm90_build LIBRARY DESTINATION sgl_kernel/sm90)
-install(TARGETS common_ops_sm100_build LIBRARY DESTINATION sgl_kernel/sm100)
+#install(TARGETS common_ops_sm100_build LIBRARY DESTINATION sgl_kernel/sm100)

 # ============================ Optional Install ============================= #
 # set flash-attention sources file
@@ -452,7 +419,7 @@
         "-O3"
         "-Xcompiler"
         "-fPIC"
-        "-gencode=arch=compute_90a,code=sm_90a"
+        "-gencode=arch=compute_87,code=sm_87"
         "-std=c++17"
         "-DCUTE_USE_PACKED_TUPLE=1"
         "-DCUTLASS_ENABLE_TENSOR_CORE_MMA=1"
@@ -469,8 +436,7 @@

     if (ENABLE_BELOW_SM90)
         list(APPEND SGL_FLASH_KERNEL_CUDA_FLAGS
-            "-gencode=arch=compute_80,code=sm_80"
-            "-gencode=arch=compute_86,code=sm_86"
+            "-gencode=arch=compute_87,code=sm_87"
         )
         # SM8X Logic
         file(GLOB FA3_SM8X_GEN_SRCS
@@ -595,13 +561,3 @@
         DESTINATION "triton_kernels"
         PATTERN ".git*" EXCLUDE
         PATTERN "__pycache__" EXCLUDE)
-
-# flash attention 4
-# TODO: find a better install condition.
-if ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "12.8" OR SGL_KERNEL_ENABLE_SM100A)
-    # flash_attn/cute
-    install(DIRECTORY "${repo-flash-attention-origin_SOURCE_DIR}/flash_attn/cute/"
-            DESTINATION "flash_attn/cute"
-            PATTERN ".git*" EXCLUDE
-            PATTERN "__pycache__" EXCLUDE)
-    endif()
