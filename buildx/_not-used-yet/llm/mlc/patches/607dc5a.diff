Submodule 3rdparty/tvm contains modified content
Submodule 3rdparty/cutlass contains modified content
diff --git a/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt b/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt
index b880de0a..efe2394c 100755
--- a/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt
@@ -133,13 +133,7 @@ set(CUTLASS_ENABLE_GTEST_UNIT_TESTS ${CUTLASS_ENABLE_TESTS} CACHE BOOL "Enable C
 
 set(CUTLASS_NVCC_ARCHS_SUPPORTED "")
 if (CUDA_VERSION VERSION_GREATER_EQUAL 11.4 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70 72 75 80 86 87)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 89 90)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 12.0 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 90a)
+  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72 87)
 endif()
 set(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING "The SM architectures requested.")
 set(CUTLASS_NVCC_ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS} CACHE STRING "The SM architectures to build code for.")
Submodule 3rdparty/cutlass_fpA_intB_gemm contains modified content
Submodule cutlass contains modified content
diff --git a/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt b/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt
index 30e261c2..bb5f2427 100755
--- a/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt
@@ -101,26 +101,8 @@ if (CUTLASS_ENABLE_TESTS)
 endif()
 
 set(CUTLASS_NVCC_ARCHS_SUPPORTED "")
-if (NOT CUDA_VERSION VERSION_LESS 7.5)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 53)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 8.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 60 61)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 9.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 9.2)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 10.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 75)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 11.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 80)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 11.1 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 86)
+if (CUDA_VERSION VERSION_GREATER_EQUAL 11.4 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
+  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72 87)
 endif()
 set(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING "The SM architectures requested.")
 set(CUTLASS_NVCC_ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS} CACHE STRING "The SM architectures to build code for.")
Submodule 3rdparty/flashinfer contains modified content
diff --git a/3rdparty/tvm/3rdparty/flashinfer/cmake/modules/FindThrust.cmake b/3rdparty/tvm/3rdparty/flashinfer/cmake/modules/FindThrust.cmake
index 5f38d41..a0f8008 100644
--- a/3rdparty/tvm/3rdparty/flashinfer/cmake/modules/FindThrust.cmake
+++ b/3rdparty/tvm/3rdparty/flashinfer/cmake/modules/FindThrust.cmake
@@ -51,13 +51,32 @@ if( THRUST_INCLUDE_DIR )
     version
     "${version}"
     )
+
+  file( STRINGS ${THRUST_INCLUDE_DIR}/thrust/version.h
+    major_version
+    REGEX "#define THRUST_MAJOR_VERSION[ \t]+([0-9x]+)"
+    )
+  string( REGEX REPLACE
+    "#define THRUST_MAJOR_VERSION[ \t]+"
+    ""
+    major_version
+    "${major_version}"
+    )
+
+  file( STRINGS ${THRUST_INCLUDE_DIR}/thrust/version.h
+    major_version
+    REGEX "#define THRUST_MINOR_VERSION[ \t]+([0-9x]+)"
+    )
+  string( REGEX REPLACE
+    "#define THRUST_MINOR_VERSION[ \t]+"
+    ""
+    minor_version
+    "${minor_version}"
+    )
   
-  math(EXPR major "${version} / 100000")
-  math(EXPR minor "(${version} / 100) % 1000")
-  math(EXPR version "${version} % 100")
-  set( THRUST_VERSION "${major}.${minor}.${version}")
-  set( THRUST_MAJOR_VERSION "${major}")
-  set( THRUST_MINOR_VERSION "${minor}")
+  set( THRUST_VERSION "${version}")
+  set( THRUST_MAJOR_VERSION "${major_version}")
+  set( THRUST_MINOR_VERSION "${minor_version}")
 endif( THRUST_INCLUDE_DIR )
 
 # Check for required components
Submodule 3rdparty/libflash_attn contains modified content
Submodule cutlass contains modified content
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt
index 2d4f9cc3..726751ec 100755
--- a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt
@@ -122,13 +122,7 @@ endif()
 
 set(CUTLASS_NVCC_ARCHS_SUPPORTED "")
 if (CUDA_VERSION VERSION_GREATER_EQUAL 11.4 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70 72 75 80 86 87)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 89 90)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 12.0 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 90a)
+  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72 87)
 endif()
 set(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING "The SM architectures requested.")
 set(CUTLASS_NVCC_ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS} CACHE STRING "The SM architectures to build code for.")
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp
index 7b3b6f4f..9fb29af1 100644
--- a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp
+++ b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp
@@ -99,7 +99,9 @@ struct ComposedLayout : private cute::tuple<LayoutA, Offset, LayoutB>  // EBO fo
   // Doesn't really make sense to ask for the strides of this "layout"
   CUTE_HOST_DEVICE constexpr
   decltype(auto)
-  stride() const = delete;
+  stride() const /*= delete;*/ {
+    return layout_b().stride();
+  }
 
   //
   // Mappings
@@ -228,7 +230,10 @@ shape(ComposedLayout<A,O,B> const& layout)
 template <int... Is, class Fn, class O, class Layout>
 CUTE_HOST_DEVICE constexpr
 decltype(auto)
-stride(ComposedLayout<Fn,O,Layout> const& layout) = delete;
+stride(ComposedLayout<Fn,O,Layout> const& layout) /*= delete;*/
+{
+  return stride<Is...>(layout.layout_b());
+}
 
 // Return the number of elements in a mode
 template <int... Is, class A, class O, class B>
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt b/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt
index ba2ac1a..97c1712 100644
--- a/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt
@@ -1,6 +1,4 @@
-set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr --expt-relaxed-constexpr --use_fast_math -t 8 \
-                      -gencode=arch=compute_80,code=\\\"sm_80,compute_80\\\" \
-                      ")
+set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr --expt-relaxed-constexpr --use_fast_math -t 8")
 
 include_directories(${CUTLASS_DIR}/include)
 include_directories(../include)
@@ -16,5 +14,3 @@ add_library(flash_attn SHARED
   flash_fwd_hdim64_fp16_sm80.cu
   flash_fwd_hdim96_fp16_sm80.cu
 )
-
-set_target_properties(flash_attn PROPERTIES CUDA_ARCHITECTURES "80")
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h b/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h
index b4a4b48..a1eef59 100644
--- a/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h
+++ b/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h
@@ -1,6 +1,10 @@
-// Inspired by https://github.com/NVIDIA/DALI/blob/main/include/dali/core/static_switch.h
+// Inspired by
+// https://github.com/NVIDIA/DALI/blob/main/include/dali/core/static_switch.h
 // and https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/Dispatch.h
-
+//
+// dusty-nv:  pulled from the patched version below due to compiler errors
+//  https://github.com/Dao-AILab/flash-attention/pull/343
+//  https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/csrc/flash_attn/src/static_switch.h
 #pragma once
 
 /// @param COND       - a boolean expression to switch by
@@ -13,53 +17,53 @@
 ///     some_function<BoolConst>(...);
 /// });
 /// ```
-#define BOOL_SWITCH(COND, CONST_NAME, ...)                                           \
-    [&] {                                                                            \
-        if (COND) {                                                                  \
-            constexpr bool CONST_NAME = true;                                        \
-            return __VA_ARGS__();                                                    \
-        } else {                                                                     \
-            constexpr bool CONST_NAME = false;                                       \
-            return __VA_ARGS__();                                                    \
-        }                                                                            \
-    }()
+#define BOOL_SWITCH(COND, CONST_NAME, ...)      \
+  [&] {                                         \
+    if (COND) {                                 \
+      constexpr static bool CONST_NAME = true;  \
+      return __VA_ARGS__();                     \
+    } else {                                    \
+      constexpr static bool CONST_NAME = false; \
+      return __VA_ARGS__();                     \
+    }                                           \
+  }()
 
-#define FP16_SWITCH(COND, ...)                     \
-    [&] {                                          \
-        if (COND) {                                \
-            using elem_type = cutlass::half_t;     \
-            return __VA_ARGS__();                  \
-        } else {                                   \
-            using elem_type = cutlass::bfloat16_t; \
-            return __VA_ARGS__();                  \
-        }                                          \
-    }()
+#define FP16_SWITCH(COND, ...)               \
+  [&] {                                      \
+    if (COND) {                              \
+      using elem_type = cutlass::half_t;     \
+      return __VA_ARGS__();                  \
+    } else {                                 \
+      using elem_type = cutlass::bfloat16_t; \
+      return __VA_ARGS__();                  \
+    }                                        \
+  }()
 
-#define FWD_HEADDIM_SWITCH(HEADDIM, ...)  \
-    [&] {                                 \
-        if (HEADDIM <= 32) {              \
-            constexpr int kHeadDim = 32;  \
-            return __VA_ARGS__();         \
-        } else if (HEADDIM <= 64) {       \
-            constexpr int kHeadDim = 64;  \
-            return __VA_ARGS__();         \
-        } else if (HEADDIM <= 96) {       \
-            constexpr int kHeadDim = 96;  \
-            return __VA_ARGS__();         \
-        } else if (HEADDIM <= 128) {      \
-            constexpr int kHeadDim = 128; \
-            return __VA_ARGS__();         \
-        } else if (HEADDIM <= 160) {      \
-            constexpr int kHeadDim = 160; \
-            return __VA_ARGS__();         \
-        } else if (HEADDIM <= 192) {      \
-            constexpr int kHeadDim = 192; \
-            return __VA_ARGS__();         \
-        } else if (HEADDIM <= 224) {      \
-            constexpr int kHeadDim = 224; \
-            return __VA_ARGS__();         \
-        } else if (HEADDIM <= 256) {      \
-            constexpr int kHeadDim = 256; \
-            return __VA_ARGS__();         \
-        }                                 \
-    }()
+#define FWD_HEADDIM_SWITCH(HEADDIM, ...)   \
+  [&] {                                    \
+    if (HEADDIM <= 32) {                   \
+      constexpr static int kHeadDim = 32;  \
+      return __VA_ARGS__();                \
+    } else if (HEADDIM <= 64) {            \
+      constexpr static int kHeadDim = 64;  \
+      return __VA_ARGS__();                \
+    } else if (HEADDIM <= 96) {            \
+      constexpr static int kHeadDim = 96;  \
+      return __VA_ARGS__();                \
+    } else if (HEADDIM <= 128) {           \
+      constexpr static int kHeadDim = 128; \
+      return __VA_ARGS__();                \
+    } else if (HEADDIM <= 160) {           \
+      constexpr static int kHeadDim = 160; \
+      return __VA_ARGS__();                \
+    } else if (HEADDIM <= 192) {           \
+      constexpr static int kHeadDim = 192; \
+      return __VA_ARGS__();                \
+    } else if (HEADDIM <= 224) {           \
+      constexpr static int kHeadDim = 224; \
+      return __VA_ARGS__();                \
+    } else if (HEADDIM <= 256) {           \
+      constexpr static int kHeadDim = 256; \
+      return __VA_ARGS__();                \
+    }                                      \
+  }()
diff --git a/3rdparty/tvm/CMakeLists.txt b/3rdparty/tvm/CMakeLists.txt
index 2186d959b..5d982dc97 100644
--- a/3rdparty/tvm/CMakeLists.txt
+++ b/3rdparty/tvm/CMakeLists.txt
@@ -913,7 +913,7 @@ if(USE_ROCM AND USE_RCCL)
 endif()
 
 
-option(USE_FLASHINFER "Build TVM with FlashInfer" OFF)
+option(USE_FLASHINFER "Build TVM with FlashInfer" ON)
 if (USE_FLASHINFER STREQUAL "ON")
   message(STATUS "Build with FlashInfer")
   set(FLASHINFER_TVM_BINDING ON)
diff --git a/3rdparty/tvm/python/setup.py b/3rdparty/tvm/python/setup.py
index 594ab5fc8..e9c73593f 100644
--- a/3rdparty/tvm/python/setup.py
+++ b/3rdparty/tvm/python/setup.py
@@ -142,7 +142,7 @@ def _remove_path(path):
 
 
 LIB_LIST, __version__ = get_lib_path()
-__version__ = git_describe_version(__version__)
+#__version__ = git_describe_version(__version__)
 
 
 def config_cython():
diff --git a/3rdparty/tvm/python/tvm/_ffi/libinfo.py b/3rdparty/tvm/python/tvm/_ffi/libinfo.py
index f32493a42..d3b639685 100644
--- a/3rdparty/tvm/python/tvm/_ffi/libinfo.py
+++ b/3rdparty/tvm/python/tvm/_ffi/libinfo.py
@@ -247,4 +247,4 @@ def find_include_path(name=None, search_path=None, optional=False):
 # We use the version of the incoming release for code
 # that is under development.
 # The following line is set by tvm/python/update_version.py
-__version__ = "0.16.dev0"
+__version__ = "0.15.0"
diff --git a/3rdparty/tvm/src/runtime/graph_executor/cuda_graph/graph_runtime_cuda_graph.cc b/3rdparty/tvm/src/runtime/graph_executor/cuda_graph/graph_runtime_cuda_graph.cc
index 5cd331807..3c0c98d44 100644
--- a/3rdparty/tvm/src/runtime/graph_executor/cuda_graph/graph_runtime_cuda_graph.cc
+++ b/3rdparty/tvm/src/runtime/graph_executor/cuda_graph/graph_runtime_cuda_graph.cc
@@ -51,7 +51,7 @@ class GraphExecutorCudaGraph : public GraphExecutor {
     TVMSetStream(dev.device_type, dev.device_id, capture_stream_);
 
     CUDA_CALL(cudaStreamBeginCapture(static_cast<cudaStream_t>(capture_stream_),
-                                     cudaStreamCaptureModeGlobal));
+                                     cudaStreamCaptureModeRelaxed));
   }
 
   /*!
diff --git a/3rdparty/tvm/version.py b/3rdparty/tvm/version.py
index b61a34e49..cf98057f9 100644
--- a/3rdparty/tvm/version.py
+++ b/3rdparty/tvm/version.py
@@ -44,7 +44,7 @@ import subprocess
 # Two tag formats are supported:
 # - vMAJ.MIN.PATCH (e.g. v0.8.0) or
 # - vMAJ.MIN.devN (e.g. v0.8.dev0)
-__version__ = "0.16.dev0"
+__version__ = "0.15.0"
 
 # ---------------------------------------------------
 
diff --git a/CMakeLists.txt b/CMakeLists.txt
index a1644f08..4a21c32a 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -47,13 +47,13 @@ set(CMAKE_POSITION_INDEPENDENT_CODE ON)
 # tvm runtime config: minimize runtime components
 set(USE_RPC OFF)
 set(USE_MICRO OFF)
-set(USE_GRAPH_EXECUTOR OFF)
+#set(USE_GRAPH_EXECUTOR OFF)
 set(USE_GRAPH_EXECUTOR_DEBUG OFF)
 set(USE_AOT_EXECUTOR OFF)
 set(USE_PROFILER OFF)
 set(USE_GTEST OFF)
 set(USE_LIBBACKTRACE OFF)
-set(BUILD_DUMMY_LIBTVM ON)
+#set(BUILD_DUMMY_LIBTVM ON)
 if (NOT DEFINED TVM_HOME)
   set(TVM_HOME 3rdparty/tvm)
 endif (NOT DEFINED TVM_HOME)
diff --git a/mlc_llm/core.py b/mlc_llm/core.py
index bd86e0a4..4f273462 100644
--- a/mlc_llm/core.py
+++ b/mlc_llm/core.py
@@ -826,8 +826,8 @@ def build_model_from_args(args: argparse.Namespace):
     cache_path = os.path.join(args.artifact_path, "mod_cache_before_build.pkl")
     args.raw_params_path = os.path.join(args.artifact_path, "raw_params")
     use_cache = args.use_cache and os.path.isfile(cache_path)
-    if args.sep_embed and args.model_category != "llama":
-        raise ValueError(f"separate embedding not supported on {args.model}")
+    #if args.sep_embed and args.model_category != "llama":
+    #    raise ValueError(f"separate embedding not supported on {args.model}")
 
     if args.model_category == "minigpt":
         # Special case for minigpt, which neither provides nor requires a configuration.
diff --git a/mlc_llm/relax_model/stablelm_3b.py b/mlc_llm/relax_model/stablelm_3b.py
index ac1c9a71..086648d1 100644
--- a/mlc_llm/relax_model/stablelm_3b.py
+++ b/mlc_llm/relax_model/stablelm_3b.py
@@ -269,7 +269,8 @@ class StableLM3bAttention(nn.Module):
         k_cache = nn.emit(
             relax.op.call_inplace_packed(
                 f_kv_cache_append,
-                args=[k_cache, squeezed_key],
+                k_cache, 
+                squeezed_key,
                 inplace_indices=[0],
                 sinfo_args=[relax.ObjectStructInfo()],
             )
@@ -277,7 +278,8 @@ class StableLM3bAttention(nn.Module):
         v_cache = nn.emit(
             relax.op.call_inplace_packed(
                 f_kv_cache_append,
-                args=[v_cache, squeezed_value],
+                v_cache, 
+                squeezed_value,
                 inplace_indices=[0],
                 sinfo_args=[relax.ObjectStructInfo()],
             )
@@ -287,14 +289,16 @@ class StableLM3bAttention(nn.Module):
         k_cache = nn.emit(
             relax.call_pure_packed(
                 f_kv_cache_view,
-                args=[k_cache, kv_cache_shape],
+                k_cache, 
+                kv_cache_shape,
                 sinfo_args=[R.Tensor(kv_cache_shape, kv_states_dtype)],
             )
         )
         v_cache = nn.emit(
             relax.call_pure_packed(
                 f_kv_cache_view,
-                args=[v_cache, kv_cache_shape],
+                v_cache, 
+                kv_cache_shape,
                 sinfo_args=[R.Tensor(kv_cache_shape, kv_states_dtype)],
             )
         )
@@ -721,7 +725,9 @@ def create_kv_cache_func(bb: relax.BlockBuilder, config: StableLM3bConfig) -> No
                     bb.emit(
                         relax.call_pure_packed(
                             f_kv_cache_create,
-                            args=[zeros, init_shape, relax.PrimValue(0)],
+                            zeros, 
+                            init_shape, 
+                            relax.PrimValue(0),
                             sinfo_args=[relax.ObjectStructInfo()],
                         )
                     )
diff --git a/python/mlc_chat/model/gemma/gemma_model.py b/python/mlc_chat/model/gemma/gemma_model.py
index 01455896..a0b23eea 100644
--- a/python/mlc_chat/model/gemma/gemma_model.py
+++ b/python/mlc_chat/model/gemma/gemma_model.py
@@ -197,7 +197,7 @@ class GemmaModel(nn.Module):
     def __init__(self, config: GemmaConfig):
         self.hidden_size = config.hidden_size
         assert config.hidden_size % config.num_attention_heads == 0
-        self.embed_tokens = GemmaEmbedding("vocab_size", config.hidden_size)
+        self.embed_tokens = GemmaEmbedding(config.vocab_size, config.hidden_size)
         self.layers = nn.ModuleList(
             [GemmaDecoderLayer(config) for _ in range(config.num_hidden_layers)]
         )
diff --git a/python/mlc_chat/model/gemma/gemma_quantization.py b/python/mlc_chat/model/gemma/gemma_quantization.py
index 28b42343..88e90684 100644
--- a/python/mlc_chat/model/gemma/gemma_quantization.py
+++ b/python/mlc_chat/model/gemma/gemma_quantization.py
@@ -6,7 +6,7 @@ from typing import Tuple
 from tvm.relax.frontend import nn
 
 from mlc_chat.loader import QuantizeMapping
-from mlc_chat.quantization import GroupQuantize, NoQuantize
+from mlc_chat.quantization import FTQuantize, GroupQuantize, NoQuantize
 
 from .gemma_model import GemmaConfig, GemmaForCausalLM
 
@@ -26,7 +26,21 @@ def group_quant(
     )
     return model, quant_map
 
-
+def ft_quant(
+    model_config: GemmaConfig,
+    quantization: FTQuantize,
+) -> Tuple[nn.Module, QuantizeMapping]:
+    """Quantize a Gemma-architecture model using FasterTransformer quantization."""
+    model: nn.Module = GemmaForCausalLM(model_config)
+    model.to(quantization.model_dtype)
+    quant_map = QuantizeMapping({}, {})
+    model = quantization.quantize_model(
+        model,
+        quant_map,
+        "",
+    )
+    return model, quant_map
+    
 def no_quant(
     model_config: GemmaConfig,
     quantization: NoQuantize,
diff --git a/python/mlc_chat/model/model.py b/python/mlc_chat/model/model.py
index 68d052c1..f702f8b7 100644
--- a/python/mlc_chat/model/model.py
+++ b/python/mlc_chat/model/model.py
@@ -106,6 +106,7 @@ MODELS: Dict[str, Model] = {
         quantize={
             "no-quant": gemma_quantization.no_quant,
             "group-quant": gemma_quantization.group_quant,
+            "ft-quant": gemma_quantization.ft_quant,
         },
     ),
     "gpt2": Model(
diff --git a/python/mlc_chat/model/phi/phi_model.py b/python/mlc_chat/model/phi/phi_model.py
index 421876d1..4f569763 100644
--- a/python/mlc_chat/model/phi/phi_model.py
+++ b/python/mlc_chat/model/phi/phi_model.py
@@ -286,7 +286,7 @@ class PhiCausalLMHead(nn.Module):
         super().__init__()
 
         self.ln = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
-        self.linear = nn.Linear(config.n_embd, "vocab_size")
+        self.linear = nn.Linear(config.n_embd, config.vocab_size)
 
     def forward(self, hidden_states: Tensor):
         hidden_states = self.ln(hidden_states)
@@ -300,7 +300,7 @@ class PhiCausalLMHead(nn.Module):
 class PhiModel(nn.Module):
     def __init__(self, config: PhiConfig) -> None:
         super().__init__()
-        self.embd = nn.Embedding("vocab_size", config.n_embd)
+        self.embd = nn.Embedding(config.vocab_size, config.n_embd)
         self.h = nn.ModuleList([PhiParallelBlock(config) for i in range(config.n_layer)])
         self.tensor_parallel_shards = config.tensor_parallel_shards
 
diff --git a/python/setup.py b/python/setup.py
index f866e9a7..8c74a8e6 100644
--- a/python/setup.py
+++ b/python/setup.py
@@ -47,7 +47,7 @@ def git_describe_version(original_version):
 
 
 LIB_LIST, __version__ = get_lib_path()
-__version__ = git_describe_version(__version__)
+__version__ = "0.1.0" #git_describe_version(__version__)
 
 
 class BinaryDistribution(Distribution):
diff --git a/setup.py b/setup.py
index b9721497..dd63fd62 100644
--- a/setup.py
+++ b/setup.py
@@ -20,7 +20,7 @@ def git_describe_version(original_version):
     return gd_version
 
 
-__version__ = git_describe_version(None)
+__version__ = "0.1.0" #git_describe_version(None)
 
 setup(
     name="mlc_llm",
diff --git a/version.py b/version.py
index c7868f84..f4944686 100644
--- a/version.py
+++ b/version.py
@@ -20,7 +20,7 @@ import subprocess
 
 # ---------------------------------------------------
 
-__version__ = "0.1.dev0"
+__version__ = "0.1.0"
 PROJ_ROOT = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))
 
 
